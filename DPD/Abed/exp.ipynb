{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47097b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from optic.models.devices import mzm, photodiode\n",
    "from optic.models.channels import linearFiberChannel\n",
    "from optic.comm.sources import bitSource\n",
    "from optic.comm.modulation import modulateGray\n",
    "from optic.comm.metrics import bert\n",
    "from optic.dsp.core import firFilter, pulseShape, upsample, pnorm, anorm\n",
    "from optic.utils import parameters, dBm2W\n",
    "from scipy.special import erfc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8503b56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, initializers\n",
    "\n",
    "\n",
    "def build_dpd_model():\n",
    "    # should i change the first dim to None as per gemeni did? - cuz i feel like the next layer would not slide accross so id need to do the windowing manually as a preproc step\n",
    "    # id say for now since it works/makes sense dont try to fix it, do the preprocessing manually and dont assume the below filter slides accross automatically.\n",
    "    # update: apparently it does slide through, you just change the batch size (inference) to 1 not N/101, for now what you have just makes sense so play around with that later.\n",
    "    # the \"1\" dimension is for features, it can be 2 for say an I/Q signal - but apparently here they made two seperate nets for I and Q so ud still use 1\n",
    "    # I think the reason why batch size is mandatory to have in CNNs is cuz usually you'd pass an infintely long signal (or too long) unlike a typical dataset.\n",
    "    # so almost always you'd wanna apply batching to reduce memory footprint.\n",
    "    # but that's different from the \"timestep\" element which is the first dimension here (the 101 i chose, but can be anything .. maybe even 500 - play around w/ it.)\n",
    "    # i mean since ill be applying windowing manually so i should get the same ooutput regardless.\n",
    "    # lets now stick to what i understand - signal of length N -> reshape to N,1 -> apply a sliding window so it's (N-101, 101, 1) -> pass to the model\n",
    "    # the thing to try for later is .... set the input shape to (None,1), and pass the input as (1, N, 1) and get your CNN to slide accross automatically for you\n",
    "    # both should yield the same result - but my QS is why would you need to do batching in the first place and why is it not necessary to do for e.g. in regular NNs?\n",
    "    # thats just purely an API design choice - nothing too crazy here.\n",
    "\n",
    "    inputs = layers.Input(shape=(None,1))\n",
    "\n",
    "\n",
    "    # QS here, why is your filer 3dimensional for a 1D operation?\n",
    "    # 1D or 2D in CNNs refer to the sliding dimension, in 1D -> it's a single one way, in 2D, it slides in the X and Y directions\n",
    "    # but that doesn't mean that your input array cant be multi-dimensional, in that case your filter would need to have a shape to basically fit on it.\n",
    "\n",
    "    # so if your X input is (T,2), your filter would be F,2 as well, so there's weight parameters in the second dimension as well.\n",
    "    # now what about the third dimension (e.g. here it's 101,1,1) - that's your filter count, sometimes you may need to capture the corellation to multiple features at once\n",
    "    # so you'd use multiple filters for that.\n",
    "    kernel_init_A = np.zeros((101, 1, 1)) \n",
    "    kernel_init_A[50, 0, 0] = 1.0\n",
    "    sec_a = layers.Conv1D(filters=1, kernel_size=101, padding='same',\n",
    "                            kernel_initializer=initializers.Constant(kernel_init_A))(inputs) # note the choice of padding matters here, 'same' adds padding so out dim is (101,1)\n",
    "\n",
    "\n",
    "\n",
    "    kernel_init_B = np.zeros((11, 1, 21))\n",
    "    kernel_init_B[5, 0, :] = 1.0  # Set the middle index (5) to 1.0 for all 21 filters\n",
    "    b_conv = layers.Conv1D(filters=21, kernel_size=11, padding='same', kernel_initializer=initializers.Constant(kernel_init_B))(sec_a)\n",
    "    x = layers.Dense(12, activation=layers.LeakyReLU(negative_slope=0.1))(b_conv)\n",
    "    x = layers.Dense(8, activation=layers.LeakyReLU(negative_slope=0.1))(x)\n",
    "    x = layers.Dense(8, activation=layers.LeakyReLU(negative_slope=0.1))(x)\n",
    "    nonlinear_out = layers.Dense(1, activation='linear')(x) # Final sum to 1 neuron\n",
    "\n",
    "    sec_b = layers.Add()([sec_a, nonlinear_out])\n",
    "\n",
    "    # kernel_init_C = np.zeros((301, 1, 1)) \n",
    "    # kernel_init_C[150, 0, 0] = 1.0\n",
    "    # section_c = layers.Conv1D(filters=1, kernel_size=301, padding='same', \n",
    "    #                           kernel_initializer=initializers.Constant(kernel_init_C))(sec_b)\n",
    "\n",
    "\n",
    "\n",
    "    outputs = sec_b\n",
    "\n",
    "    return models.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "183980d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_sliding_windows(data, window_size):\n",
    "    \"\"\"\n",
    "    Converts a 1D array into a 3D windowed dataset with the same output length.\n",
    "    \"\"\"\n",
    "    data = np.asarray(data)\n",
    "    \n",
    "    # Pad the beginning of the data with zeros \n",
    "    # (window_size - 1) pads ensures the first window contains the first element\n",
    "    padding_size = window_size - 1\n",
    "    padded_data = np.pad(data, (padding_size, 0), mode='constant', constant_values=0)\n",
    "    \n",
    "    # Now the number of windows will equal len(data)\n",
    "    num_windows = len(padded_data) - window_size + 1\n",
    "    \n",
    "    # Efficient window creation\n",
    "    windows = [padded_data[i : i + window_size] for i in range(num_windows)]\n",
    "    \n",
    "    # Convert to (Samples, Window_Size, Features)\n",
    "    X = np.array(windows)\n",
    "    return X[..., np.newaxis]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c1c47df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal as sig\n",
    "\n",
    "def align_signals(tx, rx):\n",
    "    \"\"\"\n",
    "    Finds the time delay between tx and rx using cross-correlation,\n",
    "    then truncates both arrays so they are perfectly aligned in time.\n",
    "    \"\"\"\n",
    "    # Use FFT-based correlation for speed on large arrays\n",
    "    corr = sig.correlate(rx, tx, mode='full', method='fft')\n",
    "    \n",
    "    # Calculate the delay (shift)\n",
    "    delay = np.argmax(np.abs(corr)) - (len(tx) - 1)\n",
    "    \n",
    "    if delay > 0:\n",
    "        # Rx is delayed relative to Tx\n",
    "        rx_aligned = rx[delay:]\n",
    "        tx_aligned = tx[:-delay]\n",
    "    elif delay < 0:\n",
    "        # Tx is delayed relative to Rx (rare in physical systems, but possible in DSP)\n",
    "        rx_aligned = rx[:delay]\n",
    "        tx_aligned = tx[-delay:]\n",
    "    else:\n",
    "        rx_aligned = rx\n",
    "        tx_aligned = tx\n",
    "        \n",
    "    # Make sure they are the exact same length\n",
    "    min_len = min(len(tx_aligned), len(rx_aligned))\n",
    "    \n",
    "    return tx_aligned[:min_len], rx_aligned[:min_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b659281",
   "metadata": {},
   "source": [
    "NO DPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d0783e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting simulation...\n",
      "Transmission performance metrics:\n",
      "Q-factor = 3.58 \n",
      "BER = 1.00e-04\n",
      "Pb = 1.73e-04\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# simulation parameters\n",
    "SpS = 16  # samples per symbol\n",
    "M = 2  # order of the modulation format\n",
    "Rs = 10e9  # Symbol rate\n",
    "Fs = SpS * Rs  # Signal sampling frequency (samples/second)\n",
    "Pi_dBm = 3  # laser optical power at the input of the MZM in dBm\n",
    "Pi = dBm2W(Pi_dBm)  # convert from dBm to W\n",
    "\n",
    "# Bit source parameters\n",
    "paramBits = parameters()\n",
    "paramBits.nBits = 100000  # number of bits to be generated\n",
    "paramBits.mode = 'random' # mode of the bit source \n",
    "paramBits.seed = 123      # seed for the random number generator\n",
    "\n",
    "# pulse shaping parameters\n",
    "paramPulse = parameters()\n",
    "paramPulse.pulseType = 'nrz'  # pulse shape type\n",
    "paramPulse.SpS = SpS     # samples per symbol  \n",
    "\n",
    "# MZM parameters\n",
    "paramMZM = parameters()\n",
    "paramMZM.Vpi = 2\n",
    "paramMZM.Vb = -paramMZM.Vpi / 2\n",
    "\n",
    "# linear fiber optical channel parameters\n",
    "paramCh = parameters()\n",
    "paramCh.L = 100        # total link distance [km]\n",
    "paramCh.alpha = 0.2    # fiber loss parameter [dB/km]\n",
    "paramCh.D = 16         # fiber dispersion parameter [ps/nm/km]\n",
    "paramCh.Fc = 193.1e12  # central optical frequency [Hz]\n",
    "paramCh.Fs = Fs\n",
    "\n",
    "# photodiode parameters\n",
    "paramPD = parameters()\n",
    "paramPD.ideal = False\n",
    "paramPD.B = Rs\n",
    "paramPD.Fs = Fs\n",
    "paramPD.seed = 456  # seed for the random number generator\n",
    "\n",
    "\n",
    "\n",
    "## Simulation\n",
    "print(\"\\nStarting simulation...\", end=\"\")\n",
    "\n",
    "# generate pseudo-random bit sequence\n",
    "bitsTx = bitSource(paramBits)\n",
    "\n",
    "# generate 2-PAM modulated symbol sequence\n",
    "symbTx = modulateGray(bitsTx, M, \"pam\")\n",
    "\n",
    "# upsampling\n",
    "symbolsUp = upsample(symbTx, SpS)\n",
    "\n",
    "# pulse shaping\n",
    "pulse = pulseShape(paramPulse)\n",
    "sigTx = firFilter(pulse, symbolsUp)\n",
    "sigTx = anorm(sigTx) # normalize to 1 Vpp\n",
    "\n",
    "# optical modulation\n",
    "Ai = np.sqrt(Pi)  # ideal cw laser constant envelope\n",
    "sigTxo = mzm(Ai, sigTx, paramMZM)\n",
    "\n",
    "# linear fiber channel model\n",
    "sigCh = linearFiberChannel(sigTxo, paramCh)\n",
    "\n",
    "# noisy PD (thermal noise + shot noise + bandwidth limit)\n",
    "I_Rx = photodiode(sigCh, paramPD)\n",
    "\n",
    "# capture samples in the middle of signaling intervals\n",
    "I_Rx = I_Rx[0::SpS]\n",
    "\n",
    "\n",
    "\n",
    "# calculate the BER and Q-factor\n",
    "BER, Q = bert(I_Rx, bitsTx)\n",
    "\n",
    "print(\"\\nTransmission performance metrics:\")\n",
    "print(f\"Q-factor = {Q:.2f} \")\n",
    "print(f\"BER = {BER:.2e}\")\n",
    "\n",
    "# theoretical error probability from Q-factor\n",
    "Pb = 0.5 * erfc(Q / np.sqrt(2))\n",
    "print(f\"Pb = {Pb:.2e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf3e6ef",
   "metadata": {},
   "source": [
    "#DPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ed2eb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-22 12:17:27.575426: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3\n",
      "2026-02-22 12:17:27.575454: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 24.00 GB\n",
      "2026-02-22 12:17:27.575459: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 8.88 GB\n",
      "2026-02-22 12:17:27.575472: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2026-02-22 12:17:27.575480: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2026-02-22 12:17:27.727333: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-factor = 3.53 \n",
      "BER = 1.56e-04\n",
      "Q-factor = 3.45 \n",
      "BER = 2.44e-04\n",
      "Q-factor = 3.42 \n",
      "BER = 2.98e-04\n",
      "Q-factor = 3.41 \n",
      "BER = 3.24e-04\n",
      "Q-factor = 3.42 \n",
      "BER = 3.43e-04\n",
      "Q-factor = 3.40 \n",
      "BER = 3.81e-04\n",
      "Q-factor = 3.38 \n",
      "BER = 4.27e-04\n",
      "Q-factor = 3.34 \n",
      "BER = 4.69e-04\n",
      "Q-factor = 3.31 \n",
      "BER = 4.96e-04\n",
      "Q-factor = 3.27 \n",
      "BER = 5.61e-04\n",
      "Q-factor = 3.23 \n",
      "BER = 6.37e-04\n",
      "Q-factor = 3.20 \n",
      "BER = 7.32e-04\n",
      "Q-factor = 3.16 \n",
      "BER = 8.32e-04\n",
      "Q-factor = 3.12 \n",
      "BER = 9.16e-04\n",
      "Q-factor = 3.08 \n",
      "BER = 1.05e-03\n",
      "Q-factor = 3.05 \n",
      "BER = 1.14e-03\n",
      "Q-factor = 3.03 \n",
      "BER = 1.21e-03\n",
      "Q-factor = 3.01 \n",
      "BER = 1.30e-03\n",
      "Q-factor = 2.99 \n",
      "BER = 1.36e-03\n",
      "Q-factor = 2.97 \n",
      "BER = 1.46e-03\n",
      "Q-factor = 2.96 \n",
      "BER = 1.51e-03\n",
      "Q-factor = 2.94 \n",
      "BER = 1.63e-03\n",
      "Q-factor = 2.92 \n",
      "BER = 1.73e-03\n",
      "Q-factor = 2.91 \n",
      "BER = 1.78e-03\n",
      "Q-factor = 2.88 \n",
      "BER = 1.92e-03\n",
      "Q-factor = 2.86 \n",
      "BER = 2.07e-03\n",
      "Q-factor = 2.84 \n",
      "BER = 2.26e-03\n",
      "Q-factor = 2.83 \n",
      "BER = 2.33e-03\n",
      "Q-factor = 2.81 \n",
      "BER = 2.44e-03\n",
      "Q-factor = 2.81 \n",
      "BER = 2.44e-03\n",
      "Q-factor = 2.82 \n",
      "BER = 2.37e-03\n",
      "Q-factor = 2.82 \n",
      "BER = 2.32e-03\n",
      "Q-factor = 2.82 \n",
      "BER = 2.29e-03\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 90\u001b[39m\n\u001b[32m     85\u001b[39m I_Rx = I_Rx[\u001b[32m0\u001b[39m::SpS]\n\u001b[32m     88\u001b[39m I_Rx_norm = (I_Rx - np.mean(I_Rx)) / np.std(I_Rx)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[43mdpd_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mI_Rx_norm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymbTx_dpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# currently batch size here isnt doing anything as were shaping that as 1,-1,1\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# PERFORMANCE METRICS\u001b[39;00m\n\u001b[32m     94\u001b[39m BER, Q = bert(I_Rx, bitsTx) \u001b[38;5;66;03m# BER and Q-factor\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/major_proj/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/major_proj/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:397\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    395\u001b[39m callbacks.on_epoch_begin(epoch)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator.catch_stop_iteration():\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbegin_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_train_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbegin_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/major_proj/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:764\u001b[39m, in \u001b[36mTFEpochIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    763\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_epoch_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/major_proj/.venv/lib/python3.12/site-packages/keras/src/trainers/epoch_iterator.py:125\u001b[39m, in \u001b[36mEpochIterator._enumerate_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    119\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m (\n\u001b[32m    120\u001b[39m             step,\n\u001b[32m    121\u001b[39m             step + \u001b[38;5;28mself\u001b[39m.steps_per_execution - \u001b[32m1\u001b[39m,\n\u001b[32m    122\u001b[39m             \u001b[38;5;28mself\u001b[39m._current_iterator,\n\u001b[32m    123\u001b[39m         )\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_batches \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._steps_seen >= \u001b[38;5;28mself\u001b[39m._num_batches:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m         \u001b[38;5;28mself\u001b[39m._current_iterator = \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m         \u001b[38;5;28mself\u001b[39m._steps_seen = \u001b[32m0\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/major_proj/.venv/lib/python3.12/site-packages/tensorflow/python/data/ops/dataset_ops.py:501\u001b[39m, in \u001b[36mDatasetV2.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m context.executing_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops.inside_function():\n\u001b[32m    500\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ops.colocate_with(\u001b[38;5;28mself\u001b[39m._variant_tensor):\n\u001b[32m--> \u001b[39m\u001b[32m501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    503\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33miteration in eager mode or within tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/major_proj/.venv/lib/python3.12/site-packages/tensorflow/python/data/ops/iterator_ops.py:705\u001b[39m, in \u001b[36mOwnedIterator.__init__\u001b[39m\u001b[34m(self, dataset, components, element_spec)\u001b[39m\n\u001b[32m    701\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    702\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    703\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    704\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnot be specified.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[38;5;28mself\u001b[39m._get_next_call_count = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/major_proj/.venv/lib/python3.12/site-packages/tensorflow/python/data/ops/iterator_ops.py:744\u001b[39m, in \u001b[36mOwnedIterator._create_iterator\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    741\u001b[39m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype.args[\u001b[32m0\u001b[39m].args[\u001b[32m0\u001b[39m].args) == \u001b[38;5;28mlen\u001b[39m(\n\u001b[32m    742\u001b[39m       \u001b[38;5;28mself\u001b[39m._flat_output_types)\n\u001b[32m    743\u001b[39m   \u001b[38;5;28mself\u001b[39m._iterator_resource.op.experimental_set_type(fulltype)\n\u001b[32m--> \u001b[39m\u001b[32m744\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/major_proj/.venv/lib/python3.12/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3478\u001b[39m, in \u001b[36mmake_iterator\u001b[39m\u001b[34m(dataset, iterator, name)\u001b[39m\n\u001b[32m   3476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tld.is_eager:\n\u001b[32m   3477\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3478\u001b[39m     _result = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3479\u001b[39m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMakeIterator\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3480\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m   3481\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# simulation parameters\n",
    "SpS = 16  # samples per symbol\n",
    "M = 2  # order of the modulation format\n",
    "Rs = 10e9  # Symbol rate\n",
    "Fs = SpS * Rs  # Signal sampling frequency (samples/second)\n",
    "Pi_dBm = 3  # laser optical power at the input of the MZM in dBm\n",
    "Pi = dBm2W(Pi_dBm)  # convert from dBm to W\n",
    "\n",
    "# Bit source parameters\n",
    "paramBits = parameters()\n",
    "paramBits.nBits = 2**18  # number of bits to be generated\n",
    "paramBits.mode = 'random' # mode of the bit source \n",
    "paramBits.seed = 123      # seed for the random number generator\n",
    "\n",
    "# pulse shaping parameters\n",
    "paramPulse = parameters()\n",
    "paramPulse.pulseType = 'nrz'  # pulse shape type\n",
    "paramPulse.SpS = SpS     # samples per symbol  \n",
    "\n",
    "# MZM parameters\n",
    "paramMZM = parameters()\n",
    "paramMZM.Vpi = 2\n",
    "paramMZM.Vb = -paramMZM.Vpi / 2\n",
    "\n",
    "# linear fiber optical channel parameters\n",
    "paramCh = parameters()\n",
    "paramCh.L = 100        # total link distance [km]\n",
    "paramCh.alpha = 0.2    # fiber loss parameter [dB/km]\n",
    "paramCh.D = 16         # fiber dispersion parameter [ps/nm/km]\n",
    "paramCh.Fc = 193.1e12  # central optical frequency [Hz]\n",
    "paramCh.Fs = Fs\n",
    "\n",
    "# photodiode parameters\n",
    "paramPD = parameters()\n",
    "paramPD.ideal = False\n",
    "paramPD.B = Rs\n",
    "paramPD.Fs = Fs\n",
    "paramPD.seed = 456  # seed for the random number generator\n",
    "\n",
    "\n",
    "# DPD Models:\n",
    "#dpd_model_copy = build_dpd_model()\n",
    "seq_length = 1024\n",
    "dpd_model = build_dpd_model()\n",
    "dpd_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='mse')\n",
    "\n",
    "\n",
    "BER_list = []\n",
    "Q_list = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(50):\n",
    "    ## Starting Simulation\n",
    "\n",
    "    # generate pseudo-random bit sequence\n",
    "    bitsTx = bitSource(paramBits)\n",
    "\n",
    "    # generate 2-PAM modulated symbol sequence\n",
    "    symbTx = modulateGray(bitsTx, M, \"pam\")\n",
    "\n",
    "    # symbTx_windows = create_sliding_windows(symbTx, window_size=seq_length)\n",
    "    symbTx_dpd = dpd_model.predict(symbTx.reshape(1,-1,1), verbose=0).flatten() #TODO convert into tf dataset for faster inference.\n",
    "        \n",
    "        \n",
    "    # upsampling\n",
    "    symbolsUp = upsample(symbTx_dpd, SpS)\n",
    "\n",
    "    # pulse shaping\n",
    "    pulse = pulseShape(paramPulse)\n",
    "    sigTx = firFilter(pulse, symbolsUp)\n",
    "    sigTx = anorm(sigTx) # normalize to 1 Vpp\n",
    "\n",
    "    # optical modulation\n",
    "    Ai = np.sqrt(Pi)  # ideal cw laser constant envelope\n",
    "    sigTxo = mzm(Ai, sigTx, paramMZM)\n",
    "\n",
    "    # linear fiber channel model\n",
    "    sigCh = linearFiberChannel(sigTxo, paramCh)\n",
    "\n",
    "    # noisy PD (thermal noise + shot noise + bandwidth limit)\n",
    "    I_Rx = photodiode(sigCh, paramPD)\n",
    "\n",
    "    # capture samples in the middle of signaling intervals\n",
    "    I_Rx = I_Rx[0::SpS]\n",
    "\n",
    "\n",
    "    I_Rx_norm = (I_Rx - np.mean(I_Rx)) / np.std(I_Rx)\n",
    "\n",
    "    dpd_model.fit(I_Rx_norm.reshape(1,-1,1), symbTx_dpd.reshape(1,-1,1), epochs=30, verbose=0) # currently batch size here isnt doing anything as were shaping that as 1,-1,1\n",
    "\n",
    "\n",
    "    # PERFORMANCE METRICS\n",
    "    BER, Q = bert(I_Rx, bitsTx) # BER and Q-factor\n",
    "    print(f\"Q-factor = {Q:.2f} \")\n",
    "    print(f\"BER = {BER:.2e}\")\n",
    "\n",
    "    BER_list.append(BER)\n",
    "\n",
    "    Q_list.append(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac44093",
   "metadata": {},
   "source": [
    "you must be also looking into Q-factor values not just BER. BER on itself isnt enough.\n",
    "UPDATE THE PAPER USES SNR INSTEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "748d5989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-22 12:19:14.490959: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3\n",
      "2026-02-22 12:19:14.490979: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 24.00 GB\n",
      "2026-02-22 12:19:14.490984: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 8.88 GB\n",
      "2026-02-22 12:19:14.490998: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2026-02-22 12:19:14.491006: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2026-02-22 12:19:14.691381: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter  | Q-Factor   | BER       \n",
      "------------------------------\n",
      "1     | 7.02       | 0.00e+00  \n",
      "2     | 0.27       | 3.95e-01  \n",
      "3     | 0.39       | 3.50e-01  \n",
      "4     | 0.77       | 2.23e-01  \n",
      "5     | 1.23       | 1.16e-01  \n",
      "6     | 1.37       | 9.10e-02  \n",
      "7     | 1.33       | 9.67e-02  \n",
      "8     | 1.41       | 8.33e-02  \n",
      "9     | 1.57       | 6.26e-02  \n",
      "10    | 1.72       | 4.67e-02  \n",
      "11    | 1.89       | 3.32e-02  \n",
      "12    | 2.03       | 2.45e-02  \n",
      "13    | 1.96       | 2.81e-02  \n",
      "14    | 1.90       | 3.12e-02  \n",
      "15    | 1.88       | 3.22e-02  \n",
      "16    | 2.21       | 1.56e-02  \n",
      "17    | 2.15       | 1.77e-02  \n",
      "18    | 2.25       | 1.38e-02  \n",
      "19    | 2.42       | 8.75e-03  \n",
      "20    | 2.49       | 7.27e-03  \n",
      "21    | 2.53       | 6.45e-03  \n",
      "22    | 2.58       | 5.29e-03  \n",
      "23    | 2.68       | 3.88e-03  \n",
      "24    | 2.78       | 2.85e-03  \n",
      "25    | 2.86       | 2.20e-03  \n",
      "26    | 2.96       | 1.63e-03  \n",
      "27    | 3.01       | 1.36e-03  \n",
      "28    | 3.05       | 1.17e-03  \n",
      "29    | 3.11       | 9.23e-04  \n",
      "30    | 3.15       | 7.67e-04  \n",
      "31    | 3.20       | 6.64e-04  \n",
      "32    | 3.22       | 6.18e-04  \n",
      "33    | 3.23       | 5.80e-04  \n",
      "34    | 3.24       | 5.57e-04  \n",
      "35    | 3.23       | 5.72e-04  \n",
      "36    | 3.23       | 5.65e-04  \n",
      "37    | 3.24       | 5.34e-04  \n",
      "38    | 3.29       | 4.46e-04  \n",
      "39    | 3.40       | 3.24e-04  \n",
      "40    | 3.53       | 2.37e-04  \n",
      "41    | 3.67       | 1.14e-04  \n",
      "42    | 3.80       | 6.48e-05  \n",
      "43    | 3.92       | 3.81e-05  \n",
      "44    | 3.99       | 1.53e-05  \n",
      "45    | 3.94       | 2.67e-05  \n",
      "46    | 3.92       | 2.67e-05  \n",
      "47    | 5.51       | 0.00e+00  \n",
      "48    | 4.93       | 0.00e+00  \n",
      "49    | 5.18       | 0.00e+00  \n",
      "50    | 5.55       | 0.00e+00  \n",
      "\n",
      "Training Complete.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, initializers\n",
    "from optic.models.devices import mzm, photodiode\n",
    "from optic.models.channels import linearFiberChannel\n",
    "from optic.comm.sources import bitSource\n",
    "from optic.comm.modulation import modulateGray\n",
    "from optic.comm.metrics import bert\n",
    "from optic.dsp.core import firFilter, pulseShape, upsample, anorm\n",
    "from optic.utils import parameters, dBm2W\n",
    "\n",
    "# --- 1. MODEL DEFINITIONS ---\n",
    "\n",
    "def build_dpd_model():\n",
    "    \"\"\"Builds the DPD (G) as per Fig. 3 [cite: 203, 260]\"\"\"\n",
    "    inputs = layers.Input(shape=(None, 1))\n",
    "    \n",
    "    # Section A: Linear CNN (101 taps) [cite: 260]\n",
    "    kernel_init_A = np.zeros((101, 1, 1))\n",
    "    kernel_init_A[50, 0, 0] = 1.0\n",
    "    sec_a = layers.Conv1D(1, 101, padding='same', \n",
    "                          kernel_initializer=initializers.Constant(kernel_init_A))(inputs)\n",
    "\n",
    "    # Section B: Nonlinear FFNN with shortcut [cite: 258, 261]\n",
    "    b_conv = layers.Conv1D(21, 11, padding='same')(sec_a)\n",
    "    x = layers.Dense(12, activation=layers.LeakyReLU(0.1))(b_conv)\n",
    "    x = layers.Dense(8, activation=layers.LeakyReLU(0.1))(x)\n",
    "    x = layers.Dense(8, activation=layers.LeakyReLU(0.1))(x)\n",
    "    nonlinear_out = layers.Dense(1, activation='linear')(x)\n",
    "    \n",
    "    # ResNet connection \n",
    "    sec_b = layers.Add()([sec_a, nonlinear_out])\n",
    "    return Model(inputs, sec_b, name=\"DPD_G\")\n",
    "\n",
    "def build_aux_model():\n",
    "    \"\"\"Builds the Auxiliary Channel (S) as a mirrored version [cite: 267]\"\"\"\n",
    "    inputs = layers.Input(shape=(None, 1))\n",
    "    # Mirrored Section C (301 taps) [cite: 267]\n",
    "    x = layers.Conv1D(1, 301, padding='same')(inputs)\n",
    "    # Mirrored Section B\n",
    "    x = layers.Dense(21, activation=layers.LeakyReLU(0.1))(x)\n",
    "    # Mirrored Section A (101 taps)\n",
    "    outputs = layers.Conv1D(1, 101, padding='same')(x)\n",
    "    return Model(inputs, outputs, name=\"Auxiliary_S\")\n",
    "\n",
    "# --- 2. SIMULATION PARAMETERS ---\n",
    "\n",
    "SpS, M, Rs = 16, 2, 10e9\n",
    "Fs = SpS * Rs\n",
    "Pi = dBm2W(3)\n",
    "\n",
    "paramBits = parameters(); paramBits.nBits = 2**18; paramBits.seed = 123\n",
    "paramPulse = parameters(); paramPulse.pulseType = 'nrz'; paramPulse.SpS = SpS\n",
    "paramMZM = parameters(); paramMZM.Vpi = 2; paramMZM.Vb = -1\n",
    "paramCh = parameters(); paramCh.L = 80; paramCh.alpha = 0.2; paramCh.D = 16; paramCh.Fc = 193.1e12; paramCh.Fs = Fs\n",
    "paramPD = parameters(); paramPD.ideal = False; paramPD.B = Rs; paramPD.Fs = Fs; paramPD.seed = 456\n",
    "\n",
    "# --- 3. DLA INITIALIZATION ---\n",
    "\n",
    "dpd_model = build_dpd_model()\n",
    "aux_model = build_aux_model()\n",
    "\n",
    "# Cascade: x -> DPD -> Aux -> y_est [cite: 133]\n",
    "inputs_x = layers.Input(shape=(None, 1))\n",
    "z_pred = dpd_model(inputs_x)\n",
    "y_est = aux_model(z_pred)\n",
    "dla_cascade = Model(inputs_x, y_est)\n",
    "\n",
    "# Optimizers from Table II [cite: 294]\n",
    "opt_dpd = tf.keras.optimizers.Adam(learning_rate=1e-1)\n",
    "opt_aux = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
    "\n",
    "aux_model.compile(optimizer=opt_aux, loss='mse')\n",
    "dla_cascade.compile(optimizer=opt_dpd, loss='mse')\n",
    "\n",
    "# --- 4. MAIN DLA LOOP ---\n",
    "\n",
    "print(f\"{'Iter':<5} | {'Q-Factor':<10} | {'BER':<10}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for iteration in range(50): # DLA typically converges in 9 iterations [cite: 371]\n",
    "    # Step A: Data Generation\n",
    "    bitsTx = bitSource(paramBits)\n",
    "    symbTx = modulateGray(bitsTx, M, \"pam\")\n",
    "    x_input = symbTx.reshape(1, -1, 1)\n",
    "\n",
    "    # Step B: Apply current DPD and Run Simulation\n",
    "    z_dpd = dpd_model.predict(x_input, verbose=0)\n",
    "    z_signal = z_dpd.flatten()\n",
    "\n",
    "    # Optic-Py Chain\n",
    "    symbolsUp = upsample(z_signal, SpS)\n",
    "    sigTx = firFilter(pulseShape(paramPulse), symbolsUp)\n",
    "    sigTx = anorm(sigTx) # Normalize to 1 Vpp\n",
    "    sigTxo = mzm(np.sqrt(Pi), sigTx, paramMZM)\n",
    "    sigCh = linearFiberChannel(sigTxo, paramCh)\n",
    "    I_Rx = photodiode(sigCh, paramPD)[0::SpS]\n",
    "\n",
    "    # Normalize received signal for training [cite: 222]\n",
    "    y_received = (I_Rx - np.mean(I_Rx)) / np.std(I_Rx)\n",
    "    y_received = y_received.reshape(1, -1, 1)\n",
    "\n",
    "    # --- DLA STEP 1: Train Auxiliary Channel (S) ---\n",
    "    # Goal: S(z) ≈ y [cite: 134]\n",
    "    aux_model.fit(z_dpd, y_received, epochs=10, verbose=0, batch_size=2048)\n",
    "\n",
    "    # --- DLA STEP 2: Train DPD (G) ---\n",
    "    # Goal: S(G(x)) ≈ x [cite: 137]\n",
    "    aux_model.trainable = False\n",
    "    dla_cascade.fit(x_input, x_input, epochs=10, verbose=0, batch_size=4096)\n",
    "    aux_model.trainable = True\n",
    "\n",
    "    # Performance Monitoring\n",
    "    BER, Q = bert(I_Rx, bitsTx)\n",
    "    print(f\"{iteration+1:<5} | {Q:<10.2f} | {BER:<10.2e}\")\n",
    "\n",
    "print(\"\\nTraining Complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
