{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47097b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from optic.models.devices import mzm, photodiode\n",
    "from optic.models.channels import linearFiberChannel\n",
    "from optic.comm.sources import bitSource\n",
    "from optic.comm.modulation import modulateGray\n",
    "from optic.comm.metrics import bert\n",
    "from optic.dsp.core import firFilter, pulseShape, upsample, pnorm, anorm\n",
    "from optic.utils import parameters, dBm2W\n",
    "from scipy.special import erfc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8503b56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, initializers\n",
    "\n",
    "\n",
    "def build_dpd_model():\n",
    "    # should i change the first dim to None as per gemeni did? - cuz i feel like the next layer would not slide accross so id need to do the windowing manually as a preproc step\n",
    "    # id say for now since it works/makes sense dont try to fix it, do the preprocessing manually and dont assume the below filter slides accross automatically.\n",
    "    # update: apparently it does slide through, you just change the batch size (inference) to 1 not N/101, for now what you have just makes sense so play around with that later.\n",
    "    # the \"1\" dimension is for features, it can be 2 for say an I/Q signal - but apparently here they made two seperate nets for I and Q so ud still use 1\n",
    "    # I think the reason why batch size is mandatory to have in CNNs is cuz usually you'd pass an infintely long signal (or too long) unlike a typical dataset.\n",
    "    # so almost always you'd wanna apply batching to reduce memory footprint.\n",
    "    # but that's different from the \"timestep\" element which is the first dimension here (the 101 i chose, but can be anything .. maybe even 500 - play around w/ it.)\n",
    "    # i mean since ill be applying windowing manually so i should get the same ooutput regardless.\n",
    "    # lets now stick to what i understand - signal of length N -> reshape to N,1 -> apply a sliding window so it's (N-101, 101, 1) -> pass to the model\n",
    "    # the thing to try for later is .... set the input shape to (None,1), and pass the input as (1, N, 1) and get your CNN to slide accross automatically for you\n",
    "    # both should yield the same result - but my QS is why would you need to do batching in the first place and why is it not necessary to do for e.g. in regular NNs?\n",
    "    # thats just purely an API design choice - nothing too crazy here.\n",
    "\n",
    "    inputs = layers.Input(shape=(None,1))\n",
    "\n",
    "\n",
    "    # QS here, why is your filer 3dimensional for a 1D operation?\n",
    "    # 1D or 2D in CNNs refer to the sliding dimension, in 1D -> it's a single one way, in 2D, it slides in the X and Y directions\n",
    "    # but that doesn't mean that your input array cant be multi-dimensional, in that case your filter would need to have a shape to basically fit on it.\n",
    "\n",
    "    # so if your X input is (T,2), your filter would be F,2 as well, so there's weight parameters in the second dimension as well.\n",
    "    # now what about the third dimension (e.g. here it's 101,1,1) - that's your filter count, sometimes you may need to capture the corellation to multiple features at once\n",
    "    # so you'd use multiple filters for that.\n",
    "    kernel_init_A = np.zeros((101, 1, 1)) \n",
    "    kernel_init_A[50, 0, 0] = 1.0\n",
    "    sec_a = layers.Conv1D(filters=1, kernel_size=101, padding='same',\n",
    "                            kernel_initializer=initializers.Constant(kernel_init_A))(inputs) # note the choice of padding matters here, 'same' adds padding so out dim is (101,1)\n",
    "\n",
    "\n",
    "\n",
    "    kernel_init_B = np.zeros((11, 1, 21))\n",
    "    kernel_init_B[5, 0, :] = 1.0  # Set the middle index (5) to 1.0 for all 21 filters\n",
    "    b_conv = layers.Conv1D(filters=21, kernel_size=11, padding='same', kernel_initializer=initializers.Constant(kernel_init_B))(sec_a)\n",
    "    x = layers.Dense(12, activation=layers.LeakyReLU(negative_slope=0.1))(b_conv)\n",
    "    x = layers.Dense(8, activation=layers.LeakyReLU(negative_slope=0.1))(x)\n",
    "    x = layers.Dense(8, activation=layers.LeakyReLU(negative_slope=0.1))(x)\n",
    "    nonlinear_out = layers.Dense(1, activation='linear')(x) # Final sum to 1 neuron\n",
    "\n",
    "    sec_b = layers.Add()([sec_a, nonlinear_out])\n",
    "\n",
    "    # kernel_init_C = np.zeros((301, 1, 1)) \n",
    "    # kernel_init_C[150, 0, 0] = 1.0\n",
    "    # section_c = layers.Conv1D(filters=1, kernel_size=301, padding='same', \n",
    "    #                           kernel_initializer=initializers.Constant(kernel_init_C))(sec_b)\n",
    "\n",
    "\n",
    "\n",
    "    outputs = sec_b\n",
    "\n",
    "    return models.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183980d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_sliding_windows(data, window_size):\n",
    "    \"\"\"\n",
    "    Converts a 1D array into a 3D windowed dataset with the same output length.\n",
    "    \"\"\"\n",
    "    data = np.asarray(data)\n",
    "    \n",
    "    # Pad the beginning of the data with zeros \n",
    "    # (window_size - 1) pads ensures the first window contains the first element\n",
    "    padding_size = window_size - 1\n",
    "    padded_data = np.pad(data, (padding_size, 0), mode='constant', constant_values=0)\n",
    "    \n",
    "    # Now the number of windows will equal len(data)\n",
    "    num_windows = len(padded_data) - window_size + 1\n",
    "    \n",
    "    # Efficient window creation\n",
    "    windows = [padded_data[i : i + window_size] for i in range(num_windows)]\n",
    "    \n",
    "    # Convert to (Samples, Window_Size, Features)\n",
    "    X = np.array(windows)\n",
    "    return X[..., np.newaxis]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1c47df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal as sig\n",
    "\n",
    "def align_signals(tx, rx):\n",
    "    \"\"\"\n",
    "    Finds the time delay between tx and rx using cross-correlation,\n",
    "    then truncates both arrays so they are perfectly aligned in time.\n",
    "    \"\"\"\n",
    "    # Use FFT-based correlation for speed on large arrays\n",
    "    corr = sig.correlate(rx, tx, mode='full', method='fft')\n",
    "    \n",
    "    # Calculate the delay (shift)\n",
    "    delay = np.argmax(np.abs(corr)) - (len(tx) - 1)\n",
    "    \n",
    "    if delay > 0:\n",
    "        # Rx is delayed relative to Tx\n",
    "        rx_aligned = rx[delay:]\n",
    "        tx_aligned = tx[:-delay]\n",
    "    elif delay < 0:\n",
    "        # Tx is delayed relative to Rx (rare in physical systems, but possible in DSP)\n",
    "        rx_aligned = rx[:delay]\n",
    "        tx_aligned = tx[-delay:]\n",
    "    else:\n",
    "        rx_aligned = rx\n",
    "        tx_aligned = tx\n",
    "        \n",
    "    # Make sure they are the exact same length\n",
    "    min_len = min(len(tx_aligned), len(rx_aligned))\n",
    "    \n",
    "    return tx_aligned[:min_len], rx_aligned[:min_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b659281",
   "metadata": {},
   "source": [
    "NO DPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0783e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation parameters\n",
    "SpS = 16  # samples per symbol\n",
    "M = 2  # order of the modulation format\n",
    "Rs = 10e9  # Symbol rate\n",
    "Fs = SpS * Rs  # Signal sampling frequency (samples/second)\n",
    "Pi_dBm = 3  # laser optical power at the input of the MZM in dBm\n",
    "Pi = dBm2W(Pi_dBm)  # convert from dBm to W\n",
    "\n",
    "# Bit source parameters\n",
    "paramBits = parameters()\n",
    "paramBits.nBits = 100000  # number of bits to be generated\n",
    "paramBits.mode = 'random' # mode of the bit source \n",
    "paramBits.seed = 123      # seed for the random number generator\n",
    "\n",
    "# pulse shaping parameters\n",
    "paramPulse = parameters()\n",
    "paramPulse.pulseType = 'nrz'  # pulse shape type\n",
    "paramPulse.SpS = SpS     # samples per symbol  \n",
    "\n",
    "# MZM parameters\n",
    "paramMZM = parameters()\n",
    "paramMZM.Vpi = 2\n",
    "paramMZM.Vb = -paramMZM.Vpi / 2\n",
    "\n",
    "# linear fiber optical channel parameters\n",
    "paramCh = parameters()\n",
    "paramCh.L = 100        # total link distance [km]\n",
    "paramCh.alpha = 0.2    # fiber loss parameter [dB/km]\n",
    "paramCh.D = 16         # fiber dispersion parameter [ps/nm/km]\n",
    "paramCh.Fc = 193.1e12  # central optical frequency [Hz]\n",
    "paramCh.Fs = Fs\n",
    "\n",
    "# photodiode parameters\n",
    "paramPD = parameters()\n",
    "paramPD.ideal = False\n",
    "paramPD.B = Rs\n",
    "paramPD.Fs = Fs\n",
    "paramPD.seed = 456  # seed for the random number generator\n",
    "\n",
    "\n",
    "\n",
    "## Simulation\n",
    "print(\"\\nStarting simulation...\", end=\"\")\n",
    "\n",
    "# generate pseudo-random bit sequence\n",
    "bitsTx = bitSource(paramBits)\n",
    "\n",
    "# generate 2-PAM modulated symbol sequence\n",
    "symbTx = modulateGray(bitsTx, M, \"pam\")\n",
    "\n",
    "# upsampling\n",
    "symbolsUp = upsample(symbTx, SpS)\n",
    "\n",
    "# pulse shaping\n",
    "pulse = pulseShape(paramPulse)\n",
    "sigTx = firFilter(pulse, symbolsUp)\n",
    "sigTx = anorm(sigTx) # normalize to 1 Vpp\n",
    "\n",
    "# optical modulation\n",
    "Ai = np.sqrt(Pi)  # ideal cw laser constant envelope\n",
    "sigTxo = mzm(Ai, sigTx, paramMZM)\n",
    "\n",
    "# linear fiber channel model\n",
    "sigCh = linearFiberChannel(sigTxo, paramCh)\n",
    "\n",
    "# noisy PD (thermal noise + shot noise + bandwidth limit)\n",
    "I_Rx = photodiode(sigCh, paramPD)\n",
    "\n",
    "# capture samples in the middle of signaling intervals\n",
    "I_Rx = I_Rx[0::SpS]\n",
    "\n",
    "\n",
    "\n",
    "# calculate the BER and Q-factor\n",
    "BER, Q = bert(I_Rx, bitsTx)\n",
    "\n",
    "print(\"\\nTransmission performance metrics:\")\n",
    "print(f\"Q-factor = {Q:.2f} \")\n",
    "print(f\"BER = {BER:.2e}\")\n",
    "\n",
    "# theoretical error probability from Q-factor\n",
    "Pb = 0.5 * erfc(Q / np.sqrt(2))\n",
    "print(f\"Pb = {Pb:.2e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf3e6ef",
   "metadata": {},
   "source": [
    "#DPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed2eb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # simulation parameters\n",
    "# SpS = 16  # samples per symbol\n",
    "# M = 2  # order of the modulation format\n",
    "# Rs = 10e9  # Symbol rate\n",
    "# Fs = SpS * Rs  # Signal sampling frequency (samples/second)\n",
    "# Pi_dBm = 3  # laser optical power at the input of the MZM in dBm\n",
    "# Pi = dBm2W(Pi_dBm)  # convert from dBm to W\n",
    "\n",
    "# # Bit source parameters\n",
    "# paramBits = parameters()\n",
    "# paramBits.nBits = 2**18  # number of bits to be generated\n",
    "# paramBits.mode = 'random' # mode of the bit source \n",
    "# paramBits.seed = 123      # seed for the random number generator\n",
    "\n",
    "# # pulse shaping parameters\n",
    "# paramPulse = parameters()\n",
    "# paramPulse.pulseType = 'nrz'  # pulse shape type\n",
    "# paramPulse.SpS = SpS     # samples per symbol  \n",
    "\n",
    "# # MZM parameters\n",
    "# paramMZM = parameters()\n",
    "# paramMZM.Vpi = 2\n",
    "# paramMZM.Vb = -paramMZM.Vpi / 2\n",
    "\n",
    "# # linear fiber optical channel parameters\n",
    "# paramCh = parameters()\n",
    "# paramCh.L = 100        # total link distance [km]\n",
    "# paramCh.alpha = 0.2    # fiber loss parameter [dB/km]\n",
    "# paramCh.D = 16         # fiber dispersion parameter [ps/nm/km]\n",
    "# paramCh.Fc = 193.1e12  # central optical frequency [Hz]\n",
    "# paramCh.Fs = Fs\n",
    "\n",
    "# # photodiode parameters\n",
    "# paramPD = parameters()\n",
    "# paramPD.ideal = False\n",
    "# paramPD.B = Rs\n",
    "# paramPD.Fs = Fs\n",
    "# paramPD.seed = 456  # seed for the random number generator\n",
    "\n",
    "\n",
    "# # DPD Models:\n",
    "# #dpd_model_copy = build_dpd_model()\n",
    "# seq_length = 1024\n",
    "# dpd_model = build_dpd_model()\n",
    "# dpd_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss='mse')\n",
    "\n",
    "\n",
    "# BER_list = []\n",
    "# Q_list = []\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(30):\n",
    "#     ## Starting Simulation\n",
    "\n",
    "#     # generate pseudo-random bit sequence\n",
    "#     bitsTx = bitSource(paramBits)\n",
    "\n",
    "#     # generate 2-PAM modulated symbol sequence\n",
    "#     symbTx = modulateGray(bitsTx, M, \"pam\")\n",
    "\n",
    "#     # symbTx_windows = create_sliding_windows(symbTx, window_size=seq_length)\n",
    "#     symbTx_dpd = dpd_model.predict(symbTx.reshape(1,-1,1), verbose=0).flatten() #TODO convert into tf dataset for faster inference.\n",
    "        \n",
    "        \n",
    "#     # upsampling\n",
    "#     symbolsUp = upsample(symbTx_dpd, SpS)\n",
    "\n",
    "#     # pulse shaping\n",
    "#     pulse = pulseShape(paramPulse)\n",
    "#     sigTx = firFilter(pulse, symbolsUp)\n",
    "#     sigTx = anorm(sigTx) # normalize to 1 Vpp\n",
    "\n",
    "#     # optical modulation\n",
    "#     Ai = np.sqrt(Pi)  # ideal cw laser constant envelope\n",
    "#     sigTxo = mzm(Ai, sigTx, paramMZM)\n",
    "\n",
    "#     # linear fiber channel model\n",
    "#     sigCh = linearFiberChannel(sigTxo, paramCh)\n",
    "\n",
    "#     # noisy PD (thermal noise + shot noise + bandwidth limit)\n",
    "#     I_Rx = photodiode(sigCh, paramPD)\n",
    "\n",
    "#     # capture samples in the middle of signaling intervals\n",
    "#     I_Rx = I_Rx[0::SpS]\n",
    "\n",
    "\n",
    "#     I_Rx_norm = (I_Rx - np.mean(I_Rx)) / np.std(I_Rx)\n",
    "\n",
    "#     dpd_model.fit(sigTxo[::SpS].reshape(1,-1,1), symbTx.reshape(1,-1,1), epochs=30, verbose=0) # currently batch size here isnt doing anything as were shaping that as 1,-1,1\n",
    "\n",
    "\n",
    "#     # PERFORMANCE METRICS\n",
    "#     BER, Q = bert(I_Rx, bitsTx) # BER and Q-factor\n",
    "#     print(f\"Q-factor = {Q:.2f} \")\n",
    "#     print(f\"BER = {BER:.2e}\")\n",
    "\n",
    "#     BER_list.append(BER)\n",
    "\n",
    "#     Q_list.append(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac44093",
   "metadata": {},
   "source": [
    "you must be also looking into Q-factor values not just BER. BER on itself isnt enough.\n",
    "UPDATE THE PAPER USES SNR INSTEAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1373a5d7",
   "metadata": {},
   "source": [
    "## Model Arch & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d5989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-24 10:25:26.460198: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3\n",
      "2026-02-24 10:25:26.460218: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 24.00 GB\n",
      "2026-02-24 10:25:26.460222: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 8.88 GB\n",
      "2026-02-24 10:25:26.460235: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2026-02-24 10:25:26.460243: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter  | Q-Factor   | BER       \n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-24 10:25:26.678692: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     | 4.71       | 0.00e+00  \n",
      "2     | 0.68       | 2.50e-01  \n",
      "3     | 1.24       | 1.13e-01  \n",
      "4     | 1.76       | 4.47e-02  \n",
      "5     | 2.24       | 1.53e-02  \n",
      "6     | 3.17       | 1.24e-03  \n",
      "7     | 4.10       | 5.72e-05  \n",
      "8     | 5.20       | 0.00e+00  \n",
      "9     | 5.75       | 0.00e+00  \n",
      "10    | 6.18       | 0.00e+00  \n",
      "\n",
      "Training Complete.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Note, the below training results (even for ILA above) depend heavily on randomness, so some runs you may get terrible results others\n",
    "your just lucky and your model mnaged to converge to a minima .... now if it happens once again where you cant get it to converge\n",
    "\n",
    "The following things are to try:\n",
    "* [XXXXX] start the second half of the DLA training only after like half the iterations are done, so your Aux model is somewhat trained with a deescent accuracy.\n",
    "(the reason the above is Xed cuz that will just be equavailent to just training the Aux model with more epochs ..., since z[n] wont change after the first iteration if you dont update DPD)\n",
    "\n",
    "* try various learning rates (use a LR scheduler with logarithmic scaling)\n",
    "* save the model so to save your best performing one and load it if you get a bad run (resume from there)\n",
    "* increase epoch count for both stages of the DLA\n",
    "\n",
    "the above things might also help you with ILA perf as im quite sure i replicated it right plus it worked bef.\n",
    "Update: increasing epoch count sorta worked - tho slightly overfitting now.\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, initializers\n",
    "from optic.models.devices import mzm, photodiode\n",
    "from optic.models.channels import linearFiberChannel\n",
    "from optic.comm.sources import bitSource\n",
    "from optic.comm.modulation import modulateGray\n",
    "from optic.comm.metrics import bert\n",
    "from optic.dsp.core import firFilter, pulseShape, upsample, anorm\n",
    "from optic.utils import parameters, dBm2W\n",
    "\n",
    "\n",
    "def build_dpd_model():\n",
    "    inputs = layers.Input(shape=(None, 1))\n",
    "    \n",
    "    kernel_init_A = np.zeros((101, 1, 1))\n",
    "    kernel_init_A[50, 0, 0] = 1.0\n",
    "    sec_a = layers.Conv1D(1, 101, padding='same', \n",
    "                          kernel_initializer=initializers.Constant(kernel_init_A))(inputs)\n",
    "\n",
    "    b_conv = layers.Conv1D(21, 11, padding='same')(sec_a)\n",
    "    x = layers.Dense(12, activation=layers.LeakyReLU(0.1))(b_conv)\n",
    "    x = layers.Dense(8, activation=layers.LeakyReLU(0.1))(x)\n",
    "    x = layers.Dense(8, activation=layers.LeakyReLU(0.1))(x)\n",
    "    nonlinear_out = layers.Dense(1, activation='linear')(x)\n",
    "    \n",
    "    sec_b = layers.Add()([sec_a, nonlinear_out])\n",
    "\n",
    "\n",
    "    #Third 301 taps CNN? - FN ignore as it's not a physical setup so refelctions arent present?\n",
    "\n",
    "    return Model(inputs, sec_b, name=\"DPD_G\")\n",
    "\n",
    "def build_aux_model():\n",
    "    inputs = layers.Input(shape=(None, 1))\n",
    "    # Mirrored Section C (301 taps) [cite: 267]\n",
    "    x = layers.Conv1D(1, 301, padding='same')(inputs)\n",
    "    # Mirrored Section B\n",
    "    x = layers.Dense(21, activation=layers.LeakyReLU(0.1))(x)\n",
    "    # Mirrored Section A (101 taps)\n",
    "    outputs = layers.Conv1D(1, 101, padding='same')(x)\n",
    "    return Model(inputs, outputs, name=\"Auxiliary_S\")\n",
    "\n",
    "# --- 2. SIMULATION PARAMETERS ---\n",
    "\n",
    "SpS, M, Rs = 16, 2, 10e9\n",
    "Fs = SpS * Rs\n",
    "Pi = dBm2W(3)\n",
    "\n",
    "paramBits = parameters(); paramBits.nBits = 2**18; paramBits.seed = 123\n",
    "paramPulse = parameters(); paramPulse.pulseType = 'nrz'; paramPulse.SpS = SpS\n",
    "paramMZM = parameters(); paramMZM.Vpi = 2; paramMZM.Vb = -1\n",
    "paramCh = parameters(); paramCh.L = 80; paramCh.alpha = 0.2; paramCh.D = 16; paramCh.Fc = 193.1e12; paramCh.Fs = Fs\n",
    "paramPD = parameters(); paramPD.ideal = False; paramPD.B = Rs; paramPD.Fs = Fs; paramPD.seed = 456\n",
    "\n",
    "# --- 3. DLA INITIALIZATION ---\n",
    "\n",
    "dpd_model = build_dpd_model()\n",
    "aux_model = build_aux_model()\n",
    "\n",
    "# Cascade: x -> DPD -> Aux -> y_est [cite: 133]\n",
    "inputs_x = layers.Input(shape=(None, 1))\n",
    "z_pred = dpd_model(inputs_x)\n",
    "y_est = aux_model(z_pred)\n",
    "dla_cascade = Model(inputs_x, y_est)\n",
    "\n",
    "# Optimizers from Table II [cite: 294]\n",
    "opt_dpd = tf.keras.optimizers.Adam(learning_rate=1e-1)\n",
    "opt_aux = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
    "\n",
    "aux_model.compile(optimizer=opt_aux, loss='mse')\n",
    "dla_cascade.compile(optimizer=opt_dpd, loss='mse')\n",
    "\n",
    "# --- 4. MAIN DLA LOOP ---\n",
    "\n",
    "print(f\"{'Iter':<5} | {'Q-Factor':<10} | {'BER':<10}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for iteration in range(10): # DLA typically converges in 9 iterations [cite: 371]\n",
    "    # Step A: Data Generation\n",
    "    bitsTx = bitSource(paramBits)\n",
    "    symbTx = modulateGray(bitsTx, M, \"pam\")\n",
    "    x_input = symbTx.reshape(1, -1, 1)\n",
    "\n",
    "    # Step B: Apply current DPD and Run Simulation\n",
    "    z_dpd = dpd_model.predict(x_input, verbose=0)\n",
    "    z_signal = z_dpd.flatten()\n",
    "\n",
    "    # Optic-Py Chain\n",
    "    symbolsUp = upsample(z_signal, SpS)\n",
    "    sigTx = firFilter(pulseShape(paramPulse), symbolsUp)\n",
    "    sigTx = anorm(sigTx) # Normalize to 1 Vpp\n",
    "    sigTxo = mzm(np.sqrt(Pi), sigTx, paramMZM)\n",
    "    sigCh = linearFiberChannel(sigTxo, paramCh)\n",
    "    I_Rx = photodiode(sigCh, paramPD)[0::SpS]\n",
    "\n",
    "    # Normalize received signal for training [cite: 222]\n",
    "    y_received = (I_Rx - np.mean(I_Rx)) / np.std(I_Rx)\n",
    "    y_received = y_received.reshape(1, -1, 1)\n",
    "\n",
    "    # --- DLA STEP 1: Train Auxiliary Channel (S) ---\n",
    "    # Goal: S(z) ≈ y [cite: 134]\n",
    "    aux_model.fit(z_dpd, y_received, epochs=30, verbose=0, batch_size=2048)\n",
    "\n",
    "    # --- DLA STEP 2: Train DPD (G) ---\n",
    "    # Goal: S(G(x)) ≈ x [cite: 137]\n",
    "    aux_model.trainable = False\n",
    "    dla_cascade.fit(x_input, x_input, epochs=30, verbose=0, batch_size=4096)\n",
    "    #dla_cascade.fit(dla_cascade.predict(x_input, verbose=0), z_dpd, epochs=10, verbose=0, batch_size=4096) # bad perf, exp failed, revise theory ... hypo was that this hould give same result as above\n",
    "    aux_model.trainable = True\n",
    "\n",
    "    # Performance Monitoring\n",
    "    BER, Q = bert(I_Rx, bitsTx)\n",
    "    print(f\"{iteration+1:<5} | {Q:<10.2f} | {BER:<10.2e}\")\n",
    "\n",
    "print(\"\\nTraining Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00f9a18",
   "metadata": {},
   "source": [
    "## Performance Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf5c02ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting simulation...\n",
      "Transmission performance metrics:\n",
      "Q-factor = 3.57 \n",
      "BER = 1.50e-04\n",
      "\n",
      "Starting simulation...\n",
      "Transmission performance metrics:\n",
      "Q-factor = 2.90 \n",
      "BER = 1.85e-03\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def perf_sim(DPD_flag = True):\n",
    "\n",
    "    # simulation parameters\n",
    "    SpS = 16  # samples per symbol\n",
    "    M = 2  # order of the modulation format\n",
    "    Rs = 10e9  # Symbol rate\n",
    "    Fs = SpS * Rs  # Signal sampling frequency (samples/second)\n",
    "    Pi_dBm = 3  # laser optical power at the input of the MZM in dBm\n",
    "    Pi = dBm2W(Pi_dBm)  # convert from dBm to W\n",
    "\n",
    "\n",
    "    # Bit source parameters\n",
    "    paramBits = parameters()\n",
    "    paramBits.nBits = 100000  # number of bits to be generated\n",
    "    paramBits.mode = 'random' # mode of the bit source \n",
    "    paramBits.seed = 555      # seed for the random number generator\n",
    "\n",
    "    # pulse shaping parameters\n",
    "    paramPulse = parameters()\n",
    "    paramPulse.pulseType = 'nrz'  # pulse shape type\n",
    "    paramPulse.SpS = SpS     # samples per symbol  \n",
    "\n",
    "    # MZM parameters\n",
    "    paramMZM = parameters()\n",
    "    paramMZM.Vpi = 2\n",
    "    paramMZM.Vb = -paramMZM.Vpi / 2\n",
    "\n",
    "    # linear fiber optical channel parameters\n",
    "    paramCh = parameters()\n",
    "    paramCh.L = 100        # total link distance [km]\n",
    "    paramCh.alpha = 0.2    # fiber loss parameter [dB/km]\n",
    "    paramCh.D = 16         # fiber dispersion parameter [ps/nm/km]\n",
    "    paramCh.Fc = 193.1e12  # central optical frequency [Hz]\n",
    "    paramCh.Fs = Fs\n",
    "\n",
    "    # photodiode parameters\n",
    "    paramPD = parameters()\n",
    "    paramPD.ideal = False\n",
    "    paramPD.B = Rs\n",
    "    paramPD.Fs = Fs\n",
    "    paramPD.seed = 456  # seed for the random number generator\n",
    "\n",
    "\n",
    "\n",
    "    ## Simulation\n",
    "    print(\"\\nStarting simulation...\", end=\"\")\n",
    "\n",
    "    # generate pseudo-random bit sequence\n",
    "    bitsTx = bitSource(paramBits)\n",
    "\n",
    "    # generate 2-PAM modulated symbol sequence\n",
    "    symbTx = modulateGray(bitsTx, M, \"pam\")\n",
    "\n",
    "\n",
    "    if DPD_flag:  \n",
    "        # apply DPD\n",
    "        z_dpd = dpd_model.predict(symbTx.reshape(1, -1, 1), verbose=0)\n",
    "        z_signal = z_dpd.flatten()\n",
    "        # upsampling\n",
    "        symbolsUp = upsample(z_signal, SpS)\n",
    "    else:\n",
    "        # upsampling\n",
    "        symbolsUp = upsample(symbTx, SpS)\n",
    "\n",
    "    # pulse shaping\n",
    "    pulse = pulseShape(paramPulse)\n",
    "    sigTx = firFilter(pulse, symbolsUp)\n",
    "    sigTx = anorm(sigTx) # normalize to 1 Vpp\n",
    "\n",
    "    # optical modulation\n",
    "    Ai = np.sqrt(Pi)  # ideal cw laser constant envelope\n",
    "    sigTxo = mzm(Ai, sigTx, paramMZM)\n",
    "\n",
    "    # linear fiber channel model\n",
    "    sigCh = linearFiberChannel(sigTxo, paramCh)\n",
    "\n",
    "    # noisy PD (thermal noise + shot noise + bandwidth limit)\n",
    "    I_Rx = photodiode(sigCh, paramPD)\n",
    "\n",
    "    # capture samples in the middle of signaling intervals\n",
    "    I_Rx = I_Rx[0::SpS]\n",
    "\n",
    "\n",
    "\n",
    "    # calculate the BER and Q-factor\n",
    "    BER, Q = bert(I_Rx, bitsTx)\n",
    "\n",
    "    print(\"\\nTransmission performance metrics:\")\n",
    "    print(f\"Q-factor = {Q:.2f} \")\n",
    "    print(f\"BER = {BER:.2e}\")\n",
    "\n",
    "perf_sim(DPD_flag=False)\n",
    "perf_sim(DPD_flag=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
